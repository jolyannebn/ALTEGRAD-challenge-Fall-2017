{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALTEGRAD challenge Fall 2017 - Feature Extraction\n",
    "Can you predict whether two short texts have the same meaning?\n",
    "\n",
    "https://www.kaggle.com/c/altegrad-challenge-fall-17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this competition is to predict which of the provided pairs of questions contain two questions with the same meaning.\n",
    "\n",
    "The ground truth is a set of labels supplied by human experts. This is inherently subjective, as the true meaning of sentences can not be known with certainty. Human labeling is a 'noisy' process, and different people would probably disagree. As a result, ground truth labels on this dataset should be taken as indications but not 100% accurate, and may include incorrect labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# General\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import re \n",
    "import itertools\n",
    "import operator\n",
    "import copy\n",
    "import heapq\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from scipy.spatial.distance import cosine, euclidean, jaccard\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import csv\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaMulticore\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.metrics.pairwise import cosine_distances, euclidean_distances\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from jellyfish import jaro_distance, jaro_winkler\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "import spacy\n",
    "from collections import Counter\n",
    "#In the default models, the parser is loaded and enabled as part of the standard processing pipeline. \n",
    "#If you don't need any of the syntactic information, you should disable the parser. \n",
    "#Disabling the parser will make spaCy load and run much faster.\n",
    "nlp = spacy.load('en', disable=['parser'])\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_ID</th>\n",
       "      <th>text_a_ID</th>\n",
       "      <th>text_b_ID</th>\n",
       "      <th>text_a_text</th>\n",
       "      <th>text_b_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>199954</td>\n",
       "      <td>384085</td>\n",
       "      <td>What are the some of the best novels?</td>\n",
       "      <td>What are some of the greatest novels of all ti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>128681</td>\n",
       "      <td>237407</td>\n",
       "      <td>What are the pictures that made you look twice?</td>\n",
       "      <td>What are some amazing pictures one has to see ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>170846</td>\n",
       "      <td>240621</td>\n",
       "      <td>Have the ellectoral college members ever voted...</td>\n",
       "      <td>When has the electoral college voted against t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>55110</td>\n",
       "      <td>177468</td>\n",
       "      <td>Did Ravana really have 10 heads?</td>\n",
       "      <td>Why did Ravana have 10 heads?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>425513</td>\n",
       "      <td>400256</td>\n",
       "      <td>What's a book that you feel helped you to impr...</td>\n",
       "      <td>What books or magazines should I read to impro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_ID  text_a_ID  text_b_ID  \\\n",
       "0       0     199954     384085   \n",
       "1       1     128681     237407   \n",
       "2       2     170846     240621   \n",
       "3       3      55110     177468   \n",
       "4       4     425513     400256   \n",
       "\n",
       "                                         text_a_text  \\\n",
       "0              What are the some of the best novels?   \n",
       "1    What are the pictures that made you look twice?   \n",
       "2  Have the ellectoral college members ever voted...   \n",
       "3                   Did Ravana really have 10 heads?   \n",
       "4  What's a book that you feel helped you to impr...   \n",
       "\n",
       "                                         text_b_text  target  \n",
       "0  What are some of the greatest novels of all ti...       0  \n",
       "1  What are some amazing pictures one has to see ...       0  \n",
       "2  When has the electoral college voted against t...       1  \n",
       "3                      Why did Ravana have 10 heads?       1  \n",
       "4  What books or magazines should I read to impro...       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('data/train.csv', names=['row_ID', 'text_a_ID', 'text_b_ID', 'text_a_text', 'text_b_text', 'target'])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80100, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_ID</th>\n",
       "      <th>text_a_ID</th>\n",
       "      <th>text_b_ID</th>\n",
       "      <th>text_a_text</th>\n",
       "      <th>text_b_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>245776</td>\n",
       "      <td>2705</td>\n",
       "      <td>What are the best sites to book a hotel online?</td>\n",
       "      <td>What is the best hotel booking service?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>104796</td>\n",
       "      <td>48346</td>\n",
       "      <td>How can I stop masturbation?</td>\n",
       "      <td>How can I stop doing masturbation?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>41770</td>\n",
       "      <td>383018</td>\n",
       "      <td>Which is the best way to control anger?</td>\n",
       "      <td>What is the best way to control your anger?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>81132</td>\n",
       "      <td>401393</td>\n",
       "      <td>Why is my Miniature Pinscher/Chihuahua mix afr...</td>\n",
       "      <td>Why is my Black Lab/Pitbull mix puppy afraid o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>244572</td>\n",
       "      <td>7520</td>\n",
       "      <td>How do I get rid off from porn addiction?</td>\n",
       "      <td>What is the best way to overcome an porn addic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_ID  text_a_ID  text_b_ID  \\\n",
       "0       0     245776       2705   \n",
       "1       1     104796      48346   \n",
       "2       2      41770     383018   \n",
       "3       3      81132     401393   \n",
       "4       4     244572       7520   \n",
       "\n",
       "                                         text_a_text  \\\n",
       "0    What are the best sites to book a hotel online?   \n",
       "1                       How can I stop masturbation?   \n",
       "2            Which is the best way to control anger?   \n",
       "3  Why is my Miniature Pinscher/Chihuahua mix afr...   \n",
       "4          How do I get rid off from porn addiction?   \n",
       "\n",
       "                                         text_b_text  \n",
       "0            What is the best hotel booking service?  \n",
       "1                 How can I stop doing masturbation?  \n",
       "2        What is the best way to control your anger?  \n",
       "3  Why is my Black Lab/Pitbull mix puppy afraid o...  \n",
       "4  What is the best way to overcome an porn addic...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('data/test.csv', names=['row_ID', 'text_a_ID', 'text_b_ID', 'text_a_text', 'text_b_text'])\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20179, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['target'] = 7 # False target to reuse our functions\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100279, 6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Full Dataset :\n",
    "df_all_texts = pd.concat([train, test])\n",
    "df_all_texts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text_simple(text, remove_stopwords=True, pos_filtering=True, stemming=True):\n",
    "    #print(text)\n",
    "    english_stopwords = set([stopword for stopword in stopwords.words('english')])\n",
    "    punct = set(string.punctuation)\n",
    "    punct.update([\"``\", \"`\", \"...\"])\n",
    "    text = text.lower()\n",
    "    text = ''.join(l for l in text if l not in punct) # remove punctuation (preserving intra-word dashes)\n",
    "    text = re.sub(' +',' ',text) # strip extra white space\n",
    "    text = text.strip() # strip leading and trailing white space\n",
    "    \n",
    "    # tokenize (split based on whitespace)\n",
    "    tokens = text.split(' ')\n",
    "    \n",
    "    if pos_filtering == True:\n",
    "        # POS tag and retain only nouns and adjectives\n",
    "        tagged_tokens = pos_tag(tokens)\n",
    "        tokens_keep = []\n",
    "        for item in tagged_tokens:\n",
    "            if (\n",
    "            item[1] == 'NN' or\n",
    "            item[1] == 'NNS' or\n",
    "            item[1] == 'NNP' or\n",
    "            item[1] == 'NNPS' or\n",
    "            item[1] == 'JJ' or\n",
    "            item[1] == 'JJS' or\n",
    "            item[1] == 'JJR'\n",
    "            ):\n",
    "                tokens_keep.append(item[0])\n",
    "        tokens = tokens_keep\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        # remove stopwords\n",
    "        tokens = [token for token in tokens if token not in english_stopwords and len(token)>1]\n",
    "    \n",
    "    if stemming:\n",
    "        # apply Porter's stemmer\n",
    "        stemmer = nltk.stem.PorterStemmer()\n",
    "        tokens_stemmed = list()\n",
    "        for token in tokens:\n",
    "            tokens_stemmed.append(stemmer.stem(token))\n",
    "        tokens = tokens_stemmed\n",
    "    \n",
    "    return(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction of pairs ans texts array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_pairs(train, test, remove_stopwords= True, pos_filtering=False, stemming = True):\n",
    "    texts = {}\n",
    "    pairs_train = []\n",
    "    pairs_test = []\n",
    "    y_train = []\n",
    "    y_true = []\n",
    "    ids2ind = {} # will contain the row idx of each unique text in the TFIDF matrix \n",
    "\n",
    "    for idx, l in enumerate(train.values):\n",
    "        if l[1] not in texts:\n",
    "            texts[l[1]] = clean_text_simple(l[3], remove_stopwords = remove_stopwords, \n",
    "                                            pos_filtering = pos_filtering, \n",
    "                                            stemming = stemming)\n",
    "            \n",
    "        if l[2] not in texts:\n",
    "            texts[l[2]] = clean_text_simple(l[4], remove_stopwords = remove_stopwords, \n",
    "                                            pos_filtering = pos_filtering, \n",
    "                                            stemming = stemming)\n",
    "\n",
    "        pairs_train.append([l[1], l[2]])\n",
    "        y_train.append(int(l[5]))\n",
    "\n",
    "    for idx, l in enumerate(test.values):\n",
    "        if l[1] not in texts:\n",
    "            texts[l[1]] = clean_text_simple(l[3], remove_stopwords = remove_stopwords, \n",
    "                                            pos_filtering = pos_filtering, \n",
    "                                            stemming = stemming)\n",
    "            \n",
    "        if l[2] not in texts:\n",
    "            texts[l[2]] = clean_text_simple(l[4], remove_stopwords = remove_stopwords, \n",
    "                                            pos_filtering = pos_filtering, \n",
    "                                            stemming = stemming)\n",
    "\n",
    "        pairs_test.append([l[1], l[2]])\n",
    "        y_true.append(int(l[5])) \n",
    "        \n",
    "    for qid in texts:\n",
    "        ids2ind[qid] = len(ids2ind)\n",
    "    \n",
    "    return texts, pairs_train, pairs_test, y_train, y_true, ids2ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tfIdf(texts):\n",
    "    return TfidfVectorizer().fit_transform(texts.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy tags\n",
    "https://spacy.io/usage/linguistic-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spacy_tag(sentence):\n",
    "    sentence = nlp(sentence)\n",
    "    count_tags = Counter([w.pos_ for w in sentence])\n",
    "    return count_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similarity(count_tags1, count_tags2):\n",
    "    bag_of_tags1 = list(count_tags1.keys())\n",
    "    bag_of_tags_values1 = [count_tags1.get(l) for l in bag_of_tags1]\n",
    "    bag_of_tags2 = list(count_tags2.keys())\n",
    "    bag_of_tags_values2 = [count_tags2.get(l) for l in bag_of_tags2]\n",
    "    \n",
    "    everseen = list()\n",
    "    diff = 0\n",
    "    for i, tag in enumerate(bag_of_tags1):\n",
    "        if tag in bag_of_tags2:\n",
    "            everseen.append(tag)\n",
    "            index = bag_of_tags2.index(tag)\n",
    "            diff = diff + np.abs(bag_of_tags_values1[i] - bag_of_tags_values2[index])\n",
    "        else :\n",
    "            everseen.append(tag)\n",
    "            diff = diff + bag_of_tags_values1[i]\n",
    "    \n",
    "    for i, tag in enumerate(bag_of_tags2):\n",
    "        if tag not in everseen:\n",
    "            everseen.append(tag)\n",
    "            diff = diff + bag_of_tags_values2[i]\n",
    "            \n",
    "    return diff / (np.sum(bag_of_tags_values1) + np.sum(bag_of_tags_values2)) # We normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numbers of Words in common and derived features (Word operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def common_words(q1, q2):\n",
    "    return len(set(q1).intersection(set(q2)))\n",
    "\n",
    "def diff_words(q1, q2):\n",
    "    q1 = q1.split(' ')\n",
    "    q2 = q2.split(' ')\n",
    "    everseen = list()\n",
    "    diff = 0\n",
    "    for tag in q1:\n",
    "        if tag not in q2:\n",
    "            everseen.append(tag)\n",
    "            diff = diff + 1\n",
    "        else :\n",
    "            everseen.append(tag)          \n",
    "    for tag in q2:\n",
    "        if tag not in (everseen and q2):\n",
    "            everseen.append(tag)\n",
    "            diff = diff + 1\n",
    "    return diff\n",
    "\n",
    "def total_unique_words(q1, q2):\n",
    "    return len(set(q1).union(q2))\n",
    "\n",
    "def total_unq_words_stop(q1, q2):\n",
    "    stops = set([stopword for stopword in stopwords.words('english')])\n",
    "    return len([x for x in set(q1).union(q2) if x not in stops])\n",
    "\n",
    "def wc_diff(q1, q2):\n",
    "    return abs(len(q1) - len(q2))\n",
    "\n",
    "def wc_ratio(q1, q2):\n",
    "    l1 = len(q1)*1.0 \n",
    "    l2 = len(q2)\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "    \n",
    "def wc_diff_unique(q1, q2):\n",
    "    return abs(len(set(q1)) - len(set(q2)))\n",
    "\n",
    "def wc_ratio_unique(q1, q2):\n",
    "    l1 = len(set(q1)) * 1.0\n",
    "    l2 = len(set(q2))\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "\n",
    "def wc_diff_unique_stop(q1, q2):\n",
    "    stops = set([stopword for stopword in stopwords.words('english')])\n",
    "    return abs(len([x for x in set(q1) if x not in stops]) - len([x for x in set(q2) if x not in stops]))\n",
    "\n",
    "def wc_ratio_unique_stop(q1, q2):\n",
    "    stops = set([stopword for stopword in stopwords.words('english')])\n",
    "    l1 = len([x for x in set(q1) if x not in stops])*1.0 \n",
    "    l2 = len([x for x in set(q2) if x not in stops])\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "\n",
    "def same_start_word(q1, q2):\n",
    "    if not q1 or not q2:\n",
    "        return np.nan\n",
    "    return int(q1[0] == q2[0])\n",
    "\n",
    "def same_last_word(q1, q2):\n",
    "    if not q1 or not q2:\n",
    "        return np.nan\n",
    "    return int(q1[-1] == q2[-1])\n",
    "\n",
    "def char_diff(q1, q2):\n",
    "    return abs(len(''.join(q1)) - len(''.join(q2)))\n",
    "\n",
    "def char_ratio(q1, q2):\n",
    "    l1 = len(''.join(q1)) \n",
    "    l2 = len(''.join(q2))\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "\n",
    "def char_diff_unique_stop(q1, q2):\n",
    "    stops = set([stopword for stopword in stopwords.words('english')])\n",
    "    return abs(len(''.join([x for x in set(q1) if x not in stops])) - len(''.join([x for x in set(q2) if x not in stops])))\n",
    "\n",
    "def word_match_share(q1, q2):\n",
    "    stops = set([stopword for stopword in stopwords.words('english')])\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in q1:\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in q2:\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy String Matching\n",
    "Calculate edit distances between each question pair (Levenshtein, Jaro, Jaro-Winkler, ...).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fuzzy(q1_text, q2_text):   \n",
    "    q1_tokens=q1_text.split()\n",
    "    q2_tokens=q2_text.split()\n",
    "    fuzzy_distances = np.array([\n",
    "        fuzz.ratio(q1_tokens, q2_tokens),\n",
    "        fuzz.partial_ratio(q1_tokens, q2_tokens),\n",
    "        fuzz.token_sort_ratio(q1_tokens, q2_tokens),\n",
    "        fuzz.token_set_ratio(q1_tokens, q2_tokens),\n",
    "        fuzz.partial_token_sort_ratio(q1_tokens, q2_tokens),\n",
    "    ], dtype='float')\n",
    "    \n",
    "    # Normalize to [0 - 1] range.\n",
    "    fuzzy_distances /= 100\n",
    "    \n",
    "    jelly_distances = np.array([\n",
    "        jaro_distance(q1_text, q2_text),\n",
    "        jaro_winkler(q1_text, q2_text),\n",
    "    ])\n",
    "    \n",
    "    return np.concatenate([fuzzy_distances, jelly_distances])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character N-Gram Jaccard Index\n",
    "Calculate Jaccard similarities between sets of character $n$-grams for different values of $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NGRAM_RANGE = range(2, 6)\n",
    "\n",
    "def jaccard(q1, q2):\n",
    "    wic = set(q1).intersection(set(q2))\n",
    "    uw = set(q1).union(q2)\n",
    "    if len(uw) == 0:\n",
    "        uw = [1]\n",
    "    return (len(wic) / len(uw))\n",
    "\n",
    "def get_char_ngrams(doc, n):\n",
    "    return [doc[i:i + n] for i in range(len(doc) - n + 1)]\n",
    "\n",
    "def get_jaccard_set_similarities(a, b):\n",
    "    len_intersection = len(a.intersection(b))\n",
    "    jaccard_index = len_intersection / len(a.union(b))\n",
    "    jaccard_index_norm_a = len_intersection / len(a)\n",
    "    jaccard_index_norm_b = len_intersection / len(b)\n",
    "    \n",
    "    return jaccard_index, jaccard_index_norm_a, jaccard_index_norm_b\n",
    "\n",
    "def get_jaccard_similarities(q1, q2, n):\n",
    "    if len(q1) < max(NGRAM_RANGE) and len(q2) < max(NGRAM_RANGE):\n",
    "        return 1, 1, 1\n",
    "    if len(q1) < max(NGRAM_RANGE) or len(q2) < max(NGRAM_RANGE):\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    q1_ngrams = set(get_char_ngrams(q1, n))\n",
    "    q2_ngrams = set(get_char_ngrams(q2, n))\n",
    "    return get_jaccard_set_similarities(q1_ngrams, q2_ngrams)\n",
    "\n",
    "def get_question_pair_features(q1,q2):\n",
    "    \n",
    "    features = []\n",
    "    for n in NGRAM_RANGE:\n",
    "        features.extend(get_jaccard_similarities(q1, q2, n))\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### LDA Topic Distances\n",
    "Train a Latent Dirichlet Allocation model with 300 topics on the question corpus and compute topic distances between the question pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stem_pair(pair):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    return [\n",
    "        [stemmer.stem(token) for token in texts[pair[0]].split()],\n",
    "        [stemmer.stem(token) for token in texts[pair[1]].split()],\n",
    "    ]\n",
    "\n",
    "def compute_topic_distances(q1, q2, lda_dictionary, model):\n",
    "    \n",
    "    q1_bow = lda_dictionary.doc2bow(q1)\n",
    "    q2_bow = lda_dictionary.doc2bow(q2)\n",
    "    \n",
    "    q1_topic_vec = np.array(model.get_document_topics(q1_bow, minimum_probability=0))[:, 1].reshape(1, -1)\n",
    "    q2_topic_vec = np.array(model.get_document_topics(q2_bow, minimum_probability=0))[:, 1].reshape(1, -1)\n",
    "    \n",
    "    return [\n",
    "        cosine_distances(q1_topic_vec, q2_topic_vec)[0][0],\n",
    "        euclidean_distances(q1_topic_vec, q2_topic_vec)[0][0],\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lda_init(pairs_train, pairs_test, df_all_texts):\n",
    "    NUM_TOPICS = 300\n",
    "    RANDOM_SEED = 42\n",
    "    \n",
    "    lda_tokens = list()\n",
    "    for i in range(len(pairs_train)):\n",
    "        lda_tokens.append(stem_pair(pairs_train[i]))\n",
    "    for i in range(len(pairs_test)):\n",
    "        lda_tokens.append(stem_pair(pairs_test[i]))\n",
    "        \n",
    "    lda_documents = list(np.array(lda_tokens).ravel()) # When a view is desired, arr.reshape(-1) may be preferable.\n",
    "    lda_dictionary = Dictionary(lda_documents)\n",
    "    lda_corpus = [lda_dictionary.doc2bow(document) for document in lda_documents]\n",
    "\n",
    "    model = LdaMulticore(\n",
    "        lda_corpus,\n",
    "        num_topics=NUM_TOPICS,\n",
    "        id2word=lda_dictionary,\n",
    "        random_state=RANDOM_SEED,\n",
    "    )\n",
    "\n",
    "    lda_distances = list()\n",
    "    for i in lda_tokens:\n",
    "        lda_distances.append(compute_topic_distances(i[0], i[1],lda_dictionary, model))\n",
    "\n",
    "    lda_train = np.array(lda_distances[:len(pairs_train)], dtype='float64')\n",
    "    lda_test = np.array(lda_distances[len(pairs_train):], dtype='float64')\n",
    "    \n",
    "    columns_lda=['lda_1','lda_2']\n",
    "    lda_distances = pd.DataFrame(\n",
    "    lda_distances,\n",
    "    columns=columns_lda\n",
    "    )\n",
    "    \n",
    "    return lda_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lda(q1,q2, lda_df):\n",
    "    \n",
    "    raw = lda_df[lda_df['text_a_ID'] == q1][lda_df[lda_df['text_a_ID'] == q1]['text_b_ID'] == q2]\n",
    "    raw1 = lda_df[lda_df['text_a_ID'] == q2][lda_df[lda_df['text_a_ID'] == q2]['text_b_ID'] == q1]\n",
    "\n",
    "    if(raw1.empty): return raw\n",
    "    elif(raw.empty): return raw1\n",
    "    else: return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS/NER Tag Similarity\n",
    "Derive bag-of-POS-tag (part of speech tagging) and bag-of-NER-tag (Named Entity Recognition) vectors from each question and calculate their vector distances.\n",
    "\n",
    "(POS) https://spacy.io/usage/linguistic-features#pos-tagging\n",
    "\n",
    "(NER) https://spacy.io/usage/linguistic-features#named-entities\n",
    "\n",
    "(NER) labels sequences of words in a text which are the names of things, such as person and company names, or gene and protein names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_counter(name, df_all_texts, num_raw_features, pos_tags_whitelist, ner_tags_whitelist):\n",
    "    \n",
    "    X1 = np.zeros((len(df_all_texts), num_raw_features))\n",
    "    pipe_q1 = nlp.pipe(df_all_texts[name].values, n_threads=os.cpu_count())\n",
    "\n",
    "    for i, doc in enumerate(pipe_q1):\n",
    "        pos_counter = Counter(token.pos_ for token in doc)\n",
    "        ner_counter = Counter(ent.label_ for ent in doc.ents)\n",
    "        X1[i, :] = np.array(\n",
    "            [pos_counter[pos_tag] for pos_tag in pos_tags_whitelist] +\n",
    "            [ner_counter[ner_tag] for ner_tag in ner_tags_whitelist]\n",
    "        )\n",
    "    return X1\n",
    "\n",
    "def get_vector_distances(i, X1, X2, pos_tags_whitelist, ner_tags_whitelist):\n",
    "    return [\n",
    "        # POS distances.\n",
    "        cosine(X1[i, 0:len(pos_tags_whitelist)], X2[i, 0:len(pos_tags_whitelist)]),\n",
    "        euclidean(X1[i, 0:len(pos_tags_whitelist)], X2[i, 0:len(pos_tags_whitelist)]),\n",
    "\n",
    "        # NER distances.\n",
    "        euclidean(X1[i, -len(ner_tags_whitelist):], X2[i, -len(ner_tags_whitelist):]),\n",
    "        np.abs(np.sum(X1[i, -len(ner_tags_whitelist):]) - np.sum(X2[i, -len(ner_tags_whitelist):])),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pos_ner_tags(df_all_texts):\n",
    "    pos_tags_whitelist = ['ADJ', 'ADV', 'NOUN', 'PROPN', 'NUM', 'VERB']\n",
    "    ner_tags_whitelist = ['GPE', 'LOC', 'ORG', 'NORP', 'PERSON', 'PRODUCT', 'DATE', 'TIME', 'QUANTITY', 'CARDINAL']\n",
    "\n",
    "    num_raw_features = len(pos_tags_whitelist) + len(ner_tags_whitelist)\n",
    "\n",
    "    X1 = create_counter('text_a_text', df_all_texts, num_raw_features, pos_tags_whitelist, ner_tags_whitelist)\n",
    "    X2 = create_counter('text_b_text', df_all_texts, num_raw_features, pos_tags_whitelist, ner_tags_whitelist)\n",
    "\n",
    "    df_pos_q1 = pd.DataFrame(\n",
    "        X1[:, 0:len(pos_tags_whitelist)],\n",
    "        columns=['pos_q1_' + pos_tag.lower() for pos_tag in pos_tags_whitelist])\n",
    "    df_pos_q2 = pd.DataFrame(\n",
    "        X2[:, 0:len(pos_tags_whitelist)],\n",
    "        columns=['pos_q2_' + pos_tag.lower() for pos_tag in pos_tags_whitelist])\n",
    "    df_ner_q1 = pd.DataFrame(\n",
    "        X1[:, -len(ner_tags_whitelist):],\n",
    "        columns=['ner_q1_' + ner_tag.lower() for ner_tag in ner_tags_whitelist])\n",
    "    df_ner_q2 = pd.DataFrame(\n",
    "        X2[:, -len(ner_tags_whitelist):],\n",
    "        columns=['ner_q2_' + ner_tag.lower() for ner_tag in ner_tags_whitelist])\n",
    "    \n",
    "    \n",
    "    tags_distances = list()\n",
    "    for i in list(range(len(df_all_texts))):\n",
    "        tags_distances.append(get_vector_distances(i,X1, X2, pos_tags_whitelist, ner_tags_whitelist))\n",
    "\n",
    "    tags_columns=[\n",
    "            'pos_tag_cosine',\n",
    "            'pos_tag_euclidean',\n",
    "            'ner_tag_euclidean',\n",
    "            'ner_tag_count_diff',\n",
    "        ]\n",
    "\n",
    "    tags_distances = pd.DataFrame(tags_distances, columns = tags_columns)\n",
    "\n",
    "    return tags_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tags(q1, q2, df_tags):\n",
    "    raw = df_tags[df_tags['text_a_ID'] == q1][df_tags[df_tags['text_a_ID'] == q1]['text_b_ID'] == q2]\n",
    "    raw1 = df_tags[df_tags['text_a_ID'] == q2][df_tags[df_tags['text_a_ID'] == q2]['text_b_ID'] == q1]\n",
    "\n",
    "    if(raw1.empty): return raw\n",
    "    elif(raw.empty): return raw1\n",
    "    else: return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of questions\n",
    "\n",
    "More frequent questions are more likely to be duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_question_freq(train, test):\n",
    "\n",
    "    df1 = train[['text_a_text']].copy()\n",
    "    df2 = train[['text_b_text']].copy()\n",
    "    df1_test = test[['text_a_text']].copy()\n",
    "    df2_test = test[['text_b_text']].copy()\n",
    "\n",
    "    df2.rename(columns = {'question2':'question1'},inplace=True)\n",
    "    df2_test.rename(columns = {'question2':'question1'},inplace=True)\n",
    "\n",
    "    train_questions = df1.append(df2)\n",
    "    train_questions = train_questions.append(df1_test)\n",
    "    train_questions = train_questions.append(df2_test)\n",
    "    train_questions.drop_duplicates(subset = ['text_a_text'],inplace=True)\n",
    "    train_questions.reset_index(inplace=True,drop=True)\n",
    "    questions_dict = pd.Series(train_questions.index.values,index=train_questions.text_a_text.values).to_dict()\n",
    "\n",
    "    train_cp = train.copy()\n",
    "    test_cp = test.copy()\n",
    "    train_cp.drop(['text_a_ID','text_b_ID'],axis=1,inplace=True)\n",
    "    test_cp.drop(['text_a_ID','text_b_ID'],axis=1,inplace=True)\n",
    "    test_cp['target'] = -1\n",
    "\n",
    "    comb = pd.concat([train_cp,test_cp])\n",
    "    comb['q1_hash'] = comb['text_a_text'].map(questions_dict)\n",
    "    comb['q2_hash'] = comb['text_b_text'].map(questions_dict)\n",
    "\n",
    "    q1_vc = comb.q1_hash.value_counts().to_dict()\n",
    "    q2_vc = comb.q2_hash.value_counts().to_dict()\n",
    "\n",
    "    def try_apply_dict(x,dict_to_apply):\n",
    "        try:\n",
    "            return dict_to_apply[x]\n",
    "        except KeyError:\n",
    "            return 0\n",
    "\n",
    "    comb['q1_freq'] = comb['q1_hash'].map(lambda x: try_apply_dict(x,q1_vc) + try_apply_dict(x,q2_vc))\n",
    "    comb['q2_freq'] = comb['q2_hash'].map(lambda x: try_apply_dict(x,q1_vc) + try_apply_dict(x,q2_vc))\n",
    "\n",
    "    train_comb = comb[comb['target'] >= 0][['row_ID','q1_hash','q2_hash','q1_freq','q2_freq','target']]\n",
    "    test_comb = comb[comb['target'] < 0][['row_ID','q1_hash','q2_hash','q1_freq','q2_freq']]\n",
    "    \n",
    "    train_comb.reset_index(inplace=True,drop=True)\n",
    "    test_comb.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    return train_comb.sort_index(), test_comb.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_question_intersect(train, test):\n",
    "\n",
    "    ques = pd.concat([train[['text_a_text', 'text_b_text']], test[['text_a_text', 'text_b_text']]], axis=0).reset_index(drop='index')\n",
    "\n",
    "    q_dict = defaultdict(set)\n",
    "    for i in range(ques.shape[0]):\n",
    "        q_dict[ques.text_a_text[i]].add(ques.text_b_text[i])\n",
    "        q_dict[ques.text_b_text[i]].add(ques.text_a_text[i])\n",
    "\n",
    "    def q1_q2_intersect(row):\n",
    "        return(len(set(q_dict[row['text_a_text']]).intersection(set(q_dict[row['text_b_text']]))))\n",
    "\n",
    "    train['q1_q2_intersect'] = train.apply(q1_q2_intersect, axis=1, raw=True)\n",
    "    test['q1_q2_intersect'] = test.apply(q1_q2_intersect, axis=1, raw=True)\n",
    "    \n",
    "    return train['q1_q2_intersect'], test['q1_q2_intersect']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_k_cores(texts, train, test):\n",
    "\n",
    "    stat_dico = {}\n",
    "    for an_id in texts:\n",
    "        stat_dico[an_id] = [0,0,0,0] # count_qid1, count_qid1_post, count_qid2, count_qid2_post\n",
    "\n",
    "    for idx in range(train.shape[0]):\n",
    "        qid1 = train.loc[idx,'text_a_ID']\n",
    "        qid2 = train.loc[idx,'text_b_ID']\n",
    "        the_target = train.loc[idx,'target']\n",
    "\n",
    "        stat_dico[qid1][0] += 1 # incrementation du count pour ce statement\n",
    "        stat_dico[qid2][2] += 1 # incrementation du count pour ce statement\n",
    "\n",
    "        if(the_target == 1):\n",
    "            stat_dico[qid1][1] += 1 # incrementation du count_pos pour ce statement\n",
    "            stat_dico[qid2][3] += 1 # incrementation du count_pos pour ce statement\n",
    "\n",
    "    core_dico = {}        \n",
    "    for key in stat_dico:\n",
    "        if stat_dico[key][0] != 0:\n",
    "            max_k_core_q1 = stat_dico[key][1] / stat_dico[key][0]\n",
    "        else: \n",
    "            max_k_core_q1 = 0\n",
    "\n",
    "        if stat_dico[key][2] != 0:\n",
    "            max_k_core_q2 = stat_dico[key][3] / stat_dico[key][2]\n",
    "        else:\n",
    "            max_k_core_q2 = 0\n",
    "\n",
    "        core_dico[key] = [max_k_core_q1, max_k_core_q2]\n",
    "\n",
    "    core1_col = []\n",
    "    core2_col = []\n",
    "    for i in range(train.shape[0]):\n",
    "        q1id = train.loc[i,'text_a_ID']\n",
    "        q2id = train.loc[i,'text_b_ID']\n",
    "        core1_col.append(core_dico[q1id][0])\n",
    "        core2_col.append(core_dico[q2id][1])\n",
    "    \n",
    "    train_cores = pd.DataFrame(columns=['qid1_max_kcore', 'qid2_max_kcore'])\n",
    "    train_cores['qid1_max_kcore'] = core1_col\n",
    "    train_cores['qid2_max_kcore'] = core2_col\n",
    "                          \n",
    "    core1_col = []\n",
    "    core2_col = []\n",
    "    for i in range(test.shape[0]):\n",
    "        q1id = test.loc[i,'text_a_ID']\n",
    "        q2id = test.loc[i,'text_b_ID']\n",
    "\n",
    "        core1_col.append(core_dico[q1id][0])\n",
    "        core2_col.append(core_dico[q2id][1])\n",
    "        \n",
    "    test_cores = pd.DataFrame(columns=['qid1_max_kcore', 'qid2_max_kcore'])\n",
    "    test_cores['qid1_max_kcore'] = core1_col\n",
    "    test_cores['qid2_max_kcore'] = core2_col\n",
    "    \n",
    "    return train_cores, test_cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We save into csv files some features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def save_lda_df(lda_df, name):\n",
    "    with open(name, 'w') as f:\n",
    "        f.write(\"lda_1,lda_2,text_a_ID,text_b_ID\\n\")\n",
    "        for i in range(len(lda_df)):\n",
    "            f.write(str(lda_df['lda_1'][i])\n",
    "                    +','\n",
    "                    +str(lda_df['lda_2'][i])\n",
    "                    +','\n",
    "                    +str(lda_df['text_a_ID'][i])\n",
    "                    +','\n",
    "                    +str(lda_df['text_b_ID'][i])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_df_tags(df_tags, name):\n",
    "    with open(name, 'w') as f:\n",
    "        f.write(\"pos_tag_cosine,pos_tag_euclidean,ner_tag_euclidean,ner_tag_count_diff,text_a_ID,text_b_ID\\n\")\n",
    "        for i in range(len(df_tags)):\n",
    "            f.write(str(df_tags['pos_tag_cosine'][i])\n",
    "                    +','\n",
    "                    +str(df_tags['pos_tag_euclidean'][i])\n",
    "                    +','\n",
    "                    +str(df_tags['ner_tag_euclidean'][i])\n",
    "                    +','\n",
    "                    +str(df_tags['ner_tag_count_diff'][i])\n",
    "                    +','\n",
    "                    +str(df_tags['text_a_ID'][i])\n",
    "                    +','\n",
    "                    +str(df_tags['text_b_ID'][i])+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Construction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# features name\n",
    "features=['cosine similarity (CS)',\n",
    "          'total length (TL)', \n",
    "          'difference length (DL)',\n",
    "          'sim pos tags (POS)', \n",
    "          'POS*DL',\n",
    "          'POS*TL', \n",
    "          'POS*POS*TL',\n",
    "          'POS*POS*DL',\n",
    "          'common words (CW)', \n",
    "          'CW*POS','CW*POS*DL',\n",
    "          'CW*POS*TL',\n",
    "          'fuzz ratio',\n",
    "          'fuzz partial_ratio',\n",
    "          'fuzz token_sort_ratio',\n",
    "          'fuzz token_set_ratio',\n",
    "          'fuzz partial_token_sort_ratio',\n",
    "          'jaro_distance',\n",
    "          'jaro_winkler',\n",
    "          'jaccard',\n",
    "          'jaccard_index n=2',\n",
    "          'jaccard_index_norm_a n=2',\n",
    "          'jaccard_index_norm_b n=2',\n",
    "          'jaccard_index n=3',\n",
    "          'jaccard_index_norm_a n=3', \n",
    "          'jaccard_index_norm_b n=3',\n",
    "          'jaccard_index n=4', \n",
    "          'jaccard_index_norm_a n=4', \n",
    "          'jaccard_index_norm_b n=4',\n",
    "          'jaccard_index n=5', \n",
    "          'jaccard_index_norm_a n=5', \n",
    "          'jaccard_index_norm_b n=5',\n",
    "          'pos_tag_cosine',\n",
    "          'pos_tag_euclidean',\n",
    "          'ner_tag_euclidean',\n",
    "          'ner_tag_count_diff',\n",
    "          'lda_1',\n",
    "          'lda_2',\n",
    "          'common_diff',\n",
    "          'common_min',\n",
    "          'common_max',\n",
    "          'mean_len',\n",
    "          'freq_q1',\n",
    "          'freq_q2',\n",
    "          'word_match_share',\n",
    "          'diff_words',\n",
    "          'total_unique_words',\n",
    "          'total_unq_words_stop',\n",
    "          'wc_diff',\n",
    "          'wc_ratio',\n",
    "          'wc_diff_unique',\n",
    "          'wc_ratio_unique',\n",
    "          'wc_diff_unique_stop',\n",
    "          'wc_ratio_unique_stop',\n",
    "          'same_start_word',\n",
    "          'same_last_word',\n",
    "          'char_diff',\n",
    "          'char_ratio',\n",
    "          'char_diff_unique_stop',\n",
    "          'q1_q2_intersect',\n",
    "          'qid1_max_kcore',\n",
    "          'qid2_max_kcore'\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# faire varier la construction des pairs avec steeming, ...\n",
    "def construct_data(pairs_train, A, lda_df, df_tags, train_freq, train_intersect, train_cores):\n",
    "    \n",
    "    N_train = len(pairs_train)\n",
    "    X_train = np.zeros((N_train, 62))\n",
    "    SAFE_DIV = 0.0001\n",
    "\n",
    "    for i in range(N_train):\n",
    "          \n",
    "        q1 = pairs_train[i][0]\n",
    "        q2 = pairs_train[i][1]\n",
    "        \n",
    "        X_train[i, 0] = cosine_similarity(A[ids2ind[q1], :], A[ids2ind[q2], :])  \n",
    "        X_train[i, 1] = len(texts[q1]) + len(texts[q2])\n",
    "        X_train[i, 2] = abs(len(texts[q1]) - len(texts[q2])) \n",
    "        \n",
    "         # Similarity on spacy tag: noun, verb, ...\n",
    "        X_train[i, 3] = similarity(spacy_tag(texts[q1]), spacy_tag(texts[q2]))\n",
    "        X_train[i, 4] = X_train[i, 2] * X_train[i, 3]\n",
    "        X_train[i, 5] = X_train[i, 1] * X_train[i, 3]\n",
    "        X_train[i, 6] = X_train[i, 5] * X_train[i, 3]\n",
    "        X_train[i, 7] = X_train[i, 4] * X_train[i, 3]\n",
    "        \n",
    "        X_train[i, 8] = common_words(texts[q1].split(), texts[q2].split())\n",
    "        X_train[i, 9] = X_train[i, 8] * X_train[i, 3]\n",
    "        X_train[i, 10] = X_train[i, 8] * X_train[i, 4]\n",
    "        X_train[i, 11] = X_train[i, 8] * X_train[i, 5]\n",
    "        \n",
    "        # Fuzzy distances\n",
    "        a = fuzzy(texts[q1], texts[q2])\n",
    "        for j in range(7):\n",
    "            X_train[i, 12 + j] = a[j]\n",
    "          \n",
    "        # Jaccard N-grams\n",
    "        X_train[i, 19] = jaccard(texts[q1].split(), texts[q2].split())\n",
    "        b = get_question_pair_features(texts[q1], texts[q2])\n",
    "        for j in range(12):\n",
    "            X_train[i, 20 + j] = b[j]\n",
    "            \n",
    "        # POS / NER tags\n",
    "        c = get_tags(q1, q2, df_tags)\n",
    "        tags_columns=[\n",
    "            'pos_tag_cosine',\n",
    "            'pos_tag_euclidean',\n",
    "            'ner_tag_euclidean',\n",
    "            'ner_tag_count_diff',\n",
    "        ]\n",
    "        for j in range(4):\n",
    "            X_train[i, 32 + j] = c[tags_columns[j]]\n",
    "                \n",
    "        # LDA\n",
    "        columns_lda=['lda_1','lda_2']\n",
    "        d = get_lda(q1, q2, lda_df)\n",
    "        for j in range(2):\n",
    "            X_train[i, 36 + j] = d[columns_lda[j]]\n",
    "            \n",
    "        X_train[i, 38] = X_train[i, 8] / (X_train[i, 45] + SAFE_DIV) # ratio nb words in common, nb words not shared\n",
    "        X_train[i, 39] = X_train[i, 8] / (min(len(texts[q1].split()), len(texts[q2].split())) + SAFE_DIV) # common on min\n",
    "        X_train[i, 40] = X_train[i, 8] / (max(len(texts[q1].split()), len(texts[q2].split())) + SAFE_DIV) # common on max\n",
    "        X_train[i, 41] = (len(texts[q1].split()) + len(texts[q2].split()))/2 # mean lenght of words\n",
    "            \n",
    "        # Question frequency\n",
    "        X_train[i, 42] = train_freq.loc[i]['q1_freq']\n",
    "        X_train[i, 43] = train_freq.loc[i]['q2_freq']\n",
    "        \n",
    "        # Word operation features\n",
    "        X_train[i, 44] = word_match_share(texts[q1].split(), texts[q2].split())\n",
    "        X_train[i, 45] = diff_words(texts[q1], texts[q2])\n",
    "        X_train[i, 46] = total_unique_words(texts[q1].split(), texts[q2].split())\n",
    "        X_train[i, 47] = total_unq_words_stop(texts[q1].split(), texts[q2].split())\n",
    "        X_train[i, 48] = wc_diff(texts[q1].split(), texts[q2].split())\n",
    "        X_train[i, 49] = wc_ratio(texts[q1].split(), texts[q2].split())\n",
    "        X_train[i, 50] = wc_diff_unique(texts[q1].split(), texts[q2].split())\n",
    "        X_train[i, 51] = wc_ratio_unique(texts[q1].split(), texts[q2].split())\n",
    "        X_train[i, 52] = wc_diff_unique_stop(texts[q1].split(), texts[q2].split())\n",
    "        X_train[i, 53] = wc_ratio_unique_stop(texts[q1].split(), texts[q2].split())\n",
    "        X_train[i, 54] = same_start_word(texts[q1].split(), texts[q2].split())\n",
    "        X_train[i, 55] = same_last_word(texts[q1].split(), texts[q2].split())\n",
    "        X_train[i, 56] = char_diff(texts[q1].split(), texts[q2].split())\n",
    "        X_train[i, 57] = char_ratio(texts[q1].split(), texts[q2].split())\n",
    "        X_train[i, 58] = char_diff_unique_stop(texts[q1].split(), texts[q2].split())\n",
    "                                 \n",
    "        # Questionw intersection\n",
    "        X_train[i, 59] = train_intersect.loc[i]\n",
    "                                 \n",
    "        # K-cores\n",
    "        X_train[i, 60] = train_cores.loc[i]['qid1_max_kcore']\n",
    "        X_train[i, 61] = train_cores.loc[i]['qid2_max_kcore']\n",
    "          \n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs building..\n",
      "tfIdf building..\n",
      "LDA building..\n",
      "tags building..\n"
     ]
    }
   ],
   "source": [
    "print('Pairs building..')\n",
    "texts, pairs_train, pairs_test, y_train, y_true, ids2ind = construct_pairs(train, \n",
    "                                                                          test, \n",
    "                                                                          remove_stopwords = True, \n",
    "                                                                          pos_filtering = False, \n",
    "                                                                          stemming = True)\n",
    "print('tfIdf building..')\n",
    "A = tfIdf(texts)\n",
    "\n",
    "print('LDA building..')\n",
    "lda_distances = lda_init(pairs_train, pairs_test, df_all_texts)\n",
    "lda_df = pd.DataFrame([lda_distances.lda_1.values,lda_distances.lda_2.values, df_all_texts['text_a_ID'].values, df_all_texts['text_b_ID'].values]).T \n",
    "lda_df.columns=['lda_1', 'lda_2', 'text_a_ID','text_b_ID']\n",
    "\n",
    "save_lda_df(lda_df, \"lda_df_real.csv\")\n",
    "# lda_df = pd.read_csv('lda_df_real.csv')\n",
    "\n",
    "print('tags building..')\n",
    "tags_distances = pos_ner_tags(df_all_texts)\n",
    "df_tags = pd.DataFrame([tags_distances.pos_tag_cosine.values,\n",
    "                tags_distances.pos_tag_euclidean.values, \n",
    "                tags_distances.ner_tag_euclidean.values,\n",
    "                tags_distances.ner_tag_count_diff.values,\n",
    "                df_all_texts['text_a_ID'].values,\n",
    "                df_all_texts['text_b_ID'].values]).T \n",
    "df_tags.columns = ['pos_tag_cosine', 'pos_tag_euclidean','ner_tag_euclidean','ner_tag_count_diff', 'text_a_ID','text_b_ID']\n",
    "\n",
    "save_df_tags(df_tags,\"df_tags_real.csv\")\n",
    "# df_tags = pd.read_csv('df_tags_real.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequencies building..\n",
      "Intersect building..\n",
      "K-cores building..\n"
     ]
    }
   ],
   "source": [
    "print('Frequencies building..')\n",
    "train_freq, test_freq = compute_question_freq(train, test)\n",
    "print('Intersect building..')\n",
    "train_intersect, test_intersect = compute_question_intersect(train, test)\n",
    "print('K-cores building..')\n",
    "train_cores, test_cores = compute_k_cores(texts, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features construction..\n"
     ]
    }
   ],
   "source": [
    "print('Features construction..')\n",
    "X_train = construct_data(pairs_train, A, lda_df, df_tags, train_freq, train_intersect, train_cores)\n",
    "X_test = construct_data(pairs_test, A, lda_df, df_tags, test_freq,  test_intersect, test_cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Csv Writting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train).to_csv('data/X_train_processed.csv', index=False, header=features)\n",
    "pd.DataFrame(X_test).to_csv('data/X_test_processed.csv', index=False, header=features)\n",
    "pd.DataFrame(y_train).to_csv('data/y_train_processed.csv', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
